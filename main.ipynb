{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15745caf-f359-4192-bd2c-bc8ddf42cccc",
   "metadata": {},
   "source": [
    "### <u>Basic Linear Regression</u>\n",
    "Given a labeled data set $D_{train}=\\{(x_i, y_i)\\}_{i=1}^n$, where $x_i \\in \\mathbb R^d$, $y_i \\in \\mathbb R$, and $n \\gg d$. For a given $x_i$, we make the prediction $y_i = w^Tx_i$, where we have to \"learn\" the optimal $w$ via linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91548df3-a254-41a7-b6b1-d5d563038062",
   "metadata": {},
   "source": [
    "##### <u>Imports & Reading Input</u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "105fda7c-41a7-4c02-8dc6-9219533e14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "y_train = df_train.iloc[:, 1]\n",
    "X_train = df_train.iloc[:, 2:].values\n",
    "n, d = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fdcc19-190d-4a12-9165-272da5716b2c",
   "metadata": {},
   "source": [
    "##### <u>Linear Regression</u>\n",
    "The problem looks as follows: $$\\hat w = \\arg \\max_{w \\in \\mathbb R^d} \\frac 1 {2n} \\|y - Xw\\|_2^2$$The gradient of the loss function $L(w) = \\| y - Xw \\|_2^2$ is $\\triangledown_w L(w) = \\frac 1 n (X^TX w - X^Ty)$. Note that we could simply solve the normal equations $X^TXw = X^Ty$, however, we use gradient descent for the sake for entertainment. GD will converge to a global minimum since the loss function is clearly convex.<br>\n",
    "Recall the update formula for GD with momentum: $$\\begin{aligned} w^{t+1} &= w^t + \\beta (w^{t-1} - w^t) - \\alpha \\triangledown L(w^t)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06c55c10-f647-46a2-8898-7ff4bce576ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = 10E-7, 0.0000001\n",
    "wt = np.zeros(d) \n",
    "wt1 = np.zeros(d)\n",
    "\n",
    "for i in range(100): \n",
    "    grad = (1 / n) * (X_train.T @ X_train @ wt1 - X_train.T @ y_train)\n",
    "    wt2 = wt1 + beta * (wt - wt1) - alpha * grad\n",
    "    wt = np.copy(wt1)\n",
    "    wt1 = np.copy(wt2)\n",
    "\n",
    "w = wt2\n",
    "print('w =', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a1f01-f7df-42e9-87a1-f8ecdb8d18b2",
   "metadata": {},
   "source": [
    "We observe that $w = \\frac 1 {10} [1, ..., 1]$, meaning that the scalar product $w^Tx_i$ corresponds to the commponent-wise mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867e53c-54c8-427a-9f37-560f8958bf1d",
   "metadata": {},
   "source": [
    "##### <u>Testing & Training Error<u>\n",
    "Given single $x_i \\in \\mathbb R^d$, we predict $w^Tx_i = y_i \\in \\mathbb R$. Or equivalently, given the test matrix $X_{train}$, we predict $X_{train}w = y \\in \\mathbb R^n$. We define the training error as follows:$$\\text{RMSE} = \\sqrt {\\frac 1 n \\sum_{i=1}^n (y_i - w^T x_i)^2}$$We can easily compute the Root Mean Squared Error on the test set $X_{train}, \\space y_{train}$ as follows: $$\\sqrt {\\frac 1 n \\| y_{train} - X_{train} w \\|_2^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5755f3f-355b-46ee-9253-f2e2ef29a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 5.913107707840612e-13\n"
     ]
    }
   ],
   "source": [
    "RMSE = mean_squared_error(y_train, X_train @ w) ** 0.5\n",
    "print('RMSE =', RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8462c6-13bc-41a3-978e-4fd69d123455",
   "metadata": {},
   "source": [
    "##### <u>Making Predictions</u>\n",
    "Given the unlabeled data set $D_{test}=\\{x_i\\}_{i=1}^n$, where $x_i \\in \\mathbb R^d$, we make the prediction $y_{pred} = X_{test}w$. The output is saved in `output.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc422ab9-ecc2-4f9f-9e3e-687bf68b505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "X_test = df_test.iloc[:, 1:].values\n",
    "\n",
    "y_pred = X_test @ w\n",
    "\n",
    "np.savetxt('output.csv', y_pred, header='Prediction', fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
